{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy formate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQRnib2PasOreAsNe5ISaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karndeepsingh/spacy_binary_converter/blob/main/spacy_formate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0i1yXdvKXM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o50w_RkWpcWW",
        "outputId": "d98e1c12-8260-4994-8171-a0255758cdf2"
      },
      "source": [
        "pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.7\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (58.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting thinc<8.1.0,>=8.0.8\n",
            "  Downloading thinc-8.0.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 55.1 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.2 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.9 typer-0.3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "Tfh6LPjipcZZ",
        "outputId": "45266525-107b-4204-d363-ecd972c47b10"
      },
      "source": [
        "!pip install -U pip setuptools wheel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 24.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-58.0.0-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-21.2.4 setuptools-58.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW2tmztw0t6C",
        "outputId": "1e18b764-3627-4d9f-ae93-db9caa126201"
      },
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.1.2                         \n",
            "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
            "Platform         Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Python version   3.7.11                        \n",
            "Pipelines        en_core_web_sm (3.1.0)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RhPaI0To04S",
        "outputId": "09ddcf94-871b-4cdd-a86a-aaa3f265c0c2"
      },
      "source": [
        "!git clone https://github.com/walidamamou/relation_extraction_transformer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'relation_extraction_transformer'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 34 (delta 11), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYJ3NfwNpbpt",
        "outputId": "9f75bacf-8972-4453-b165-1bcd1f8c8617"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.6 MB 74 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (58.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rEXMytWpKb7",
        "outputId": "8fdf125d-1bd7-422e-a2ca-5486e7047cc3"
      },
      "source": [
        "!pip install typer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typer in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynn_3bmYpJJD"
      },
      "source": [
        "import json\n",
        "\n",
        "import typer\n",
        "from pathlib import Path\n",
        "\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from wasabi import Printer\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.util import compile_infix_regex\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "\n",
        "msg = Printer()\n",
        "\n",
        "SYMM_LABELS = [\"Binds\"]\n",
        "MAP_LABELS = {\n",
        "    \"DEGREE_IN\": \"DEGREE_IN\",\n",
        "    \"EXPERIENCE_IN\": \"EXPERIENCE_IN\"\n",
        "}\n",
        "\n",
        "ann = \"/content/relation_extraction_transformer/relations_training.txt\"\n",
        "train_file='/content/sample_data/relations_training.spacy'\n",
        "dev_file='/content/sample_data/relations_dev.spacy'\n",
        "test_file='/content/sample_data/relations_test.spacy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRzMvPVMpcmH",
        "outputId": "bd7acfaf-43f2-4e73-eb61-d407828a3ecd"
      },
      "source": [
        "Doc.set_extension(\"rel\", default={},force=True)\n",
        "vocab = Vocab()\n",
        "\n",
        "docs = {\"train\": [], \"dev\": [], \"test\": [], \"total\": []}\n",
        "ids = {\"train\": set(), \"dev\": set(), \"test\": set(), \"total\":set()}\n",
        "count_all = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n",
        "count_pos = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n",
        "with open(\"/content/relation_extraction_transformer/relations_training.txt\", encoding=\"utf8\") as jsonfile:\n",
        "    file = json.load(jsonfile)\n",
        "# for example in file:\n",
        "    example = file[0]\n",
        "    span_starts = set()\n",
        "    neg = 0\n",
        "    pos = 0\n",
        "            # Parse the tokens\n",
        "    tokens=nlp(example[\"document\"])    \n",
        "\n",
        "    spaces=[]\n",
        "    spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "    words = [t.text for t in tokens]\n",
        "    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "\n",
        "    # # Parse the GGP entities\n",
        "    spans = example[\"tokens\"]\n",
        "\n",
        "    entities = []\n",
        "    span_end_to_start = {}\n",
        "    for span in spans:\n",
        "        # print(span)\n",
        "        entity = doc.char_span(\n",
        "              span[\"start\"], span[\"end\"], label=span[\"entityLabel\"]\n",
        "          )\n",
        "        # print(entity)\n",
        "\n",
        "\n",
        "        span_end_to_start[span[\"token_start\"]] = span[\"token_start\"]\n",
        "        # print(span_end_to_start)\n",
        "        entities.append(entity)\n",
        "        span_starts.add(span[\"token_start\"])\n",
        "\n",
        "    doc.ents = entities\n",
        "    print(doc.ents)\n",
        "                # Parse the relations\n",
        "    rels = {}\n",
        "    for x1 in span_starts:\n",
        "        for x2 in span_starts:\n",
        "            rels[(x1, x2)] = {}\n",
        "    relations = example[\"relations\"]\n",
        "    print(rels)\n",
        "    for relation in relations:\n",
        "            # the 'head' and 'child' annotations refer to the end token in the span\n",
        "            # but we want the first token\n",
        "            start = span_end_to_start[relation[\"head\"]]\n",
        "            end = span_end_to_start[relation[\"child\"]]\n",
        "            label = relation[\"relationLabel\"]\n",
        "            # print(rels[(start, end)])\n",
        "            # print(label)\n",
        "            #label = MAP_LABELS[label]\n",
        "            if label not in rels[(start, end)]:\n",
        "                rels[(start, end)][label] = 1.0\n",
        "                pos += 1\n",
        "                # print(pos)\n",
        "                # print(rels[(start, end)])\n",
        "\n",
        "                # The annotation is complete, so fill in zero's where the data is missing\n",
        "    for x1 in span_starts:\n",
        "        for x2 in span_starts:\n",
        "            for label in MAP_LABELS.values():\n",
        "                if label not in rels[(x1, x2)]:\n",
        "                    neg += 1\n",
        "                    rels[(x1, x2)][label] = 0.0\n",
        "\n",
        "                    # print(rels[(x1, x2)])\n",
        "    doc._.rel = rels\n",
        "    print(rels)\n",
        "                # only keeping documents with at least 1 positive case\n",
        "    if pos > 0:\n",
        "            docs[\"total\"].append(doc)\n",
        "            count_pos[\"total\"] += pos\n",
        "            count_all[\"total\"] += pos + neg\n",
        "print(docs[\"total\"])            \n",
        "docbin = DocBin(docs = docs[\"total\"], store_user_data=True)\n",
        "docbin.to_disk(train_file)\n",
        "msg.info(\n",
        "    f\"{len(docs['total'])} training sentences\"\n",
        ")\n",
        "\n",
        "\n",
        "# main(ann, train_file, dev_file, test_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(MS/Ph.D., CS/EE)\n",
            "{(0, 0): {}, (0, 6): {}, (6, 0): {}, (6, 6): {}}\n",
            "{(0, 0): {'DEGREE_IN': 0.0, 'EXPERIENCE_IN': 0.0}, (0, 6): {'DEGREE_IN': 1.0, 'EXPERIENCE_IN': 0.0}, (6, 0): {'DEGREE_IN': 0.0, 'EXPERIENCE_IN': 0.0}, (6, 6): {'DEGREE_IN': 0.0, 'EXPERIENCE_IN': 0.0}}\n",
            "[MS/Ph.D. in CS/EE or related areas is required. Strong ability and effectiveness working in a significant technical problem domain, in the term of plan, design, execution, continuous release and service operation. Strong software engineering fundamentals, including coding, problem solving and data analysis skills. Background in machine learning/deep learning (strongly preferred). Passionate and self-motivated. Ability to effectively work in collaborative multiple project team environment and ship production features in a fast-paced environment. Good communication skills, both verbal and written. Customer/end result/Metrics driven in design and development. Strong ability in self-learning, entering new domain, managing through uncertainty in an innovative team environment]\n",
            "\u001b[38;5;4mℹ 1 training sentences\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JDwe-FepchT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_NVIpDdpce8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1s8UtdSo-ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6745363-8593-48ef-8803-9b482bfaf3a9"
      },
      "source": [
        "import json\n",
        "\n",
        "import typer\n",
        "from pathlib import Path\n",
        "\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from wasabi import Printer\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.util import compile_infix_regex\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "msg = Printer()\n",
        "\n",
        "SYMM_LABELS = [\"Binds\"]\n",
        "MAP_LABELS = {\n",
        "    \"ROLE\": \"ROLE\",\n",
        "    \"ADDRESS\": \"ADDRESS\"\n",
        "}\n",
        "\n",
        "ann = \"/content/project-3-at-2021-09-06-15-50-79eb1b47.json\"\n",
        "train_file='./relations_training.spacy'\n",
        "dev_file='./relations_dev.spacy'\n",
        "test_file='./relations_test.spacy'\n",
        "# def main(json_loc: Path, train_file: Path, dev_file: Path, test_file: Path):\n",
        "Doc.set_extension(\"rel\", default={},force=True)\n",
        "vocab = Vocab()\n",
        "\n",
        "docs = {\"train\": [], \"dev\": [], \"test\": [], \"total\": []}\n",
        "ids = {\"train\": set(), \"dev\": set(), \"test\": set(), \"total\":set()}\n",
        "count_all = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n",
        "count_pos = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n",
        "\n",
        "with open(\"/content/project-3-at-2021-09-06-15-50-79eb1b47.json\") as jsonfile:\n",
        "    file = json.load(jsonfile)\n",
        "    for example in file:\n",
        "        span_starts = set()\n",
        "        neg = 0\n",
        "        pos = 0\n",
        "        # Parse the tokens\n",
        "        \n",
        "        data = example[\"data\"]\n",
        "        print(main_doc)\n",
        "        tokens=nlp(data[\"text\"])\n",
        "        main_doc_index= {token.text : token.i for token in tokens}\n",
        "        \n",
        "        # token_start = main_doc_index[tokens[0].text]\n",
        "        # token_end = main_doc_index[tokens[0].text] + len_tokens\n",
        "        # print(main_doc_index)\n",
        "        # print(token_start, token_end)\n",
        "        annotation = example[\"annotations\"]\n",
        "        \n",
        "        \n",
        "        \n",
        "          \n",
        "\n",
        "        spaces=[]\n",
        "        spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "        words = [t.text for t in tokens]\n",
        "        doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "        \n",
        "        # print(annotation[0][\"result\"])\n",
        "        # Parse the GGP entities\n",
        "        spans = annotation[0][\"result\"]\n",
        "        entities = []\n",
        "        span_end_to_start = {}\n",
        "        id_token = {}\n",
        "        start_span_token = {}\n",
        "        end_span_token = {}\n",
        "        # print(spans)\n",
        "        for span in spans:\n",
        "            \n",
        "            if \"value\" in span.keys():\n",
        "              # print(span[\"value\"][\"start\"],span[\"value\"][\"end\"])\n",
        "              span_token=nlp(span[\"value\"][\"text\"])\n",
        "              len_span_tokens = len(span_token)\n",
        "              if len_span_tokens > 1 :\n",
        "                token_start = main_doc_index[span_token[0].text]\n",
        "                token_end = main_doc_index[span_token[0].text] + len_span_tokens\n",
        "              else:\n",
        "                token_start = main_doc_index[span_token[0].text]\n",
        "                token_end = main_doc_index[span_token[0].text] \n",
        "\n",
        "              start_span_token[span[\"value\"][\"text\"]] = token_start\n",
        "              # print(token_start, token_end)\n",
        "              \n",
        "              entity = doc.char_span(\n",
        "                    int(span[\"value\"][\"start\"]), \n",
        "                    int(span[\"value\"][\"end\"]),\n",
        "                    label = str(span[\"value\"][\"labels\"][0])\n",
        "                )\n",
        "\n",
        "\n",
        "              span_end_to_start[token_start] = token_start\n",
        "              id_token[span[\"id\"]] = span[\"value\"][\"text\"]\n",
        "              # print(span_end_to_start)\n",
        "              entities.append(entity)\n",
        "              span_starts.add(token_start)\n",
        "              \n",
        "\n",
        "        doc.ents = entities\n",
        "        # print(doc.ents)\n",
        "        # print(span_starts)\n",
        "        # print(id_token)\n",
        "        # print(start_span_token)\n",
        "\n",
        "        # Parse the relations\n",
        "        rels = {}\n",
        "        for x1 in span_starts:\n",
        "            for x2 in span_starts:\n",
        "                rels[(x1, x2)] = {}\n",
        "        # print(rels)\n",
        "\n",
        "        #print(len(relations))\n",
        "        for relation in spans:\n",
        "          if \"from_id\" in relation.keys():\n",
        "            # the 'head' and 'child' annotations refer to the end token in the span\n",
        "            # but we want the first token\n",
        "            start =start_span_token[id_token[relation[\"from_id\"]]]\n",
        "            print(start)\n",
        "            end= start_span_token[id_token[relation[\"to_id\"]]]\n",
        "            print(end)\n",
        "            # start = span_end_to_start[relation[\"from_id\"]]\n",
        "            # end = span_end_to_start[relation[\"to_id\"]]\n",
        "            label = relation[\"labels\"][0]\n",
        "            #print(rels[(start, end)])\n",
        "            #print(label)\n",
        "            #label = MAP_LABELS[label]\n",
        "            # print(start,end)\n",
        "            if label not in rels[(start, end)]:\n",
        "                rels[(start, end)][label] = 1.0\n",
        "                pos += 1\n",
        "                #print(pos)\n",
        "                #print(rels[(start, end)])\n",
        "\n",
        "        # The annotation is complete, so fill in zero's where the data is missing\n",
        "        for x1 in span_starts:\n",
        "            for x2 in span_starts:\n",
        "                for label in MAP_LABELS.values():\n",
        "                    if label not in rels[(x1, x2)]:\n",
        "                        neg += 1\n",
        "                        rels[(x1, x2)][label] = 0.0\n",
        "\n",
        "                        #print(rels[(x1, x2)])\n",
        "        doc._.rel = rels\n",
        "        print(doc._.rel)\n",
        "\n",
        "        # only keeping documents with at least 1 positive case\n",
        "        if pos > 0:\n",
        "                docs[\"total\"].append(doc)\n",
        "                count_pos[\"total\"] += pos\n",
        "                count_all[\"total\"] += pos + neg\n",
        "\n",
        "                \n",
        "                \n",
        "print(len(docs[\"total\"]))\n",
        "print(docs[\"total\"])\n",
        "docbin = DocBin(docs = docs[\"total\"], store_user_data=True)\n",
        "docbin.to_disk(train_file)\n",
        "msg.info(\n",
        "    f\"{len(docs['total'])} training sentences\"\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is in France\n",
            "3\n",
            "0\n",
            "{(0, 0): {'ROLE': 0.0, 'ADDRESS': 0.0}, (0, 3): {'ROLE': 0.0, 'ADDRESS': 0.0}, (3, 0): {'ADDRESS': 1.0, 'ROLE': 0.0}, (3, 3): {'ROLE': 0.0, 'ADDRESS': 0.0}}\n",
            "Paris is in France\n",
            "3\n",
            "0\n",
            "{(0, 0): {'ROLE': 0.0, 'ADDRESS': 0.0}, (0, 3): {'ROLE': 0.0, 'ADDRESS': 0.0}, (3, 0): {'ADDRESS': 1.0, 'ROLE': 0.0}, (3, 3): {'ROLE': 0.0, 'ADDRESS': 0.0}}\n",
            "Paris is in France\n",
            "21\n",
            "8\n",
            "35\n",
            "21\n",
            "{(8, 8): {'ROLE': 0.0, 'ADDRESS': 0.0}, (8, 35): {'ROLE': 0.0, 'ADDRESS': 0.0}, (8, 21): {'ROLE': 0.0, 'ADDRESS': 0.0}, (35, 8): {'ROLE': 0.0, 'ADDRESS': 0.0}, (35, 35): {'ROLE': 0.0, 'ADDRESS': 0.0}, (35, 21): {'ADDRESS': 1.0, 'ROLE': 0.0}, (21, 8): {'ROLE': 1.0, 'ADDRESS': 0.0}, (21, 35): {'ROLE': 0.0, 'ADDRESS': 0.0}, (21, 21): {'ROLE': 0.0, 'ADDRESS': 0.0}}\n",
            "Paris is in France\n",
            "1\n",
            "11\n",
            "0\n",
            "1\n",
            "{(0, 0): {'ROLE': 0.0, 'ADDRESS': 0.0}, (0, 1): {'ROLE': 1.0, 'ADDRESS': 0.0}, (0, 11): {'ROLE': 0.0, 'ADDRESS': 0.0}, (1, 0): {'ROLE': 0.0, 'ADDRESS': 0.0}, (1, 1): {'ROLE': 0.0, 'ADDRESS': 0.0}, (1, 11): {'ADDRESS': 1.0, 'ROLE': 0.0}, (11, 0): {'ROLE': 0.0, 'ADDRESS': 0.0}, (11, 1): {'ROLE': 0.0, 'ADDRESS': 0.0}, (11, 11): {'ROLE': 0.0, 'ADDRESS': 0.0}}\n",
            "4\n",
            "[Paris is in France, London is in UK, Relationships are everywhere, be it with your family, with your significant other, with friends, or with your pet/plant. Or in this particular case, between entity mentions within paragraphs of text. The associations within real-life relationships are pretty much well-defined (eg. mother-daughter, father-son etc), whereas the relationships between entities in a paragraph of text would require significantly more thought to extract and hence, will be the focus of this article., BORROWER Synergy One Lending, Inc. dba Mutual of Omaha Mortgage 5716 Corsa Avenuew, Sulle 102 Westlake village.]\n",
            "\u001b[38;5;4mℹ 4 training sentences\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "RWe9JQGHZfsA",
        "outputId": "212fff3d-7018-4e47-e21f-c1b67eca9871"
      },
      "source": [
        "main(ann, train_file, dev_file, test_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-e9baeed084f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-106-40e0fd90e2c2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(json_loc, train_file, dev_file, test_file)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m           \u001b[0mmain_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m           \u001b[0mmain_doc_index\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmain_doc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \"\"\"\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             )\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     def update(\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got dict)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiFLDaBOAxuy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}